{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Net\n",
    "import torch\n",
    "import config\n",
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load(config.MODEL_PATH, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - https://learn.microsoft.com/en-us/windows/ai/windows-ml/tutorials/pytorch-convert-model\n",
    " \n",
    " - It's important to call model.eval()  before exporting the model, as this sets the model to inference mode. \n",
    " - This is needed since operators like dropout or batchnorm behave differently in inference and training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv_1): Conv2d(3, 128, kernel_size=(6, 6), stride=(1, 1), padding=(1, 1))\n",
       "  (pool_1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_2): Conv2d(128, 64, kernel_size=(6, 6), stride=(2, 2), padding=(1, 1))\n",
       "  (pool_2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_3): Conv2d(64, 32, kernel_size=(6, 6), stride=(2, 2), padding=(1, 1))\n",
       "  (pool_3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop_1): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=2048, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (drop_2): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=60, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to ONNX format\n",
    "- https://pytorch.org/docs/stable/onnx.html#example-alexnet-from-pytorch-to-onnx\n",
    "\n",
    " - In onnx export we should we give an expected dummy input or example input by model, \n",
    " - in pytorch the input image array to the model should be in format NCHW\n",
    " - batch N, channels C, height H, width W.\n",
    " - since we are making prediction only for one image at time in an api request\n",
    " - batch size is kept 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 300, 300])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels = 3\n",
    "dummy_input  = torch.randn(config.BATCH_SIZE, channels, config.IMAGE_HEIGHT, config.IMAGE_WIDTH)\n",
    "dummy_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://pytorch.org/docs/stable/onnx.html#example-alexnet-from-pytorch-to-onnx\n",
    "\n",
    "- Providing input (`actual_input`) and output names (`output`) sets the display names for values within the model's graph. \n",
    "- Setting these does not change the semantics of the graph; it is only for readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"model.onnx\",\n",
    "    input_names=[\"actual_input\"],\n",
    "    output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can view the model architecture by uploading onnx file to \n",
    "- https://netron.app/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the onnx format\n",
    "\n",
    "\n",
    "- After the model is saved to onnx format we dont need pytorch or anyother deeplearning frameworks to make inference\n",
    "- https://onnxruntime.ai/docs/api/python/api_summary.html#inferencesession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random input\n",
    "test_input = np.random.randn(1, 3, 300, 300).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ort_session.run(None, {\"actual_input\": test_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.9428433]], dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - At inference we will using our custom sigmoid function, since we are not using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigmoid(x):\n",
    "    \"\"\"\n",
    "    to apply sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12533582576367583"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sigmoid(outputs[0].flatten()[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9071810b75997d435645a22a051ded0264bddfbc9d8da280b368996b30ad632"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('dev_virtual_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
